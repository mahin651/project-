

HADOOP ARCHITECTURE
HADOOP IS A FRAMEWORK(JAVA BASED) WHICH RUNS WITH THE HELP OF SERVICES CALLED DAEMONS [JPS --> JAVA PROCESSES], CLASSIFIED AS MASTER/SLAVE DAEMONS.

2 FRAMEWORKS
1. STORAGE: HDFS [HADOOP DISTRIBUTED FILE SYSTEM]
2. PROCESSING: YARN [YET ANOTHER RESOURCE NEGOTIATOR]

HDFS
HDFS IS A LOGICAL NAMESPACE ON TOP OF OPERATING SYSTEM, WHICH RUNS WITH THE HELP OF MASTER/SLAVE DAEMONS AND RESPONSIBLE FOR STORING, STRIPPING AND MIRRORING THE DATA.

HDFS RESEMBLES LINUX, AND ALSO RECOMMENDED OS IS LINUX.

HDFS DAEMONS
MASTER --> NAMENODE
SLAVE --> DATANODE

WE CAN KNOW ABOUT THE METADATA INFORMATION FROM NAMENODE WEBUI (BROWSER BASED INTERFACE WITH A PORT NUMBER)

NAMENODE WEBUI: http://<hostname>:50070

IF REPLICATION FACTOR IS 3
UNDER REPLICATION < 3

NAMENODE WILL AUTOMATICALLY WILL INSTRUCT ONE OF THE DATANODE HOLDING THE COPY OF THE DATA TO WRITE TO ANOTHER NODE AND ELIMINATE THE UNDER REPLICATION.

OVER REPLICATION > 3

NAMENODE WILL INSTRUCT THE DATANODE TO REMOVE ITS EXCESS BLOCKS TO ELIMINATE OVER REPLICATION.

NON HA (HIGH AVAILABILITY)
1. SINGLE POINT OF FAILURE.
2. IT IS IMPLEMENTED FOR DEVELOPMENT AND LEARNING.
3. SINGLE NAMENODE.
4. CANNOT BE USING IN PRODUCTIONS.
5. WE HAVE ADDITIONAL DAEMON CALLED "SECONDARY NAMENODE" FOR METADATA MERGE.
6. SECONDARY NAMENODE IS NOT A BACKUP OR STANDBY, IT IS JUST A HELPING HAND FOR NAMENODE.

HA
1. THERE WILL BE STANDBY NAMENODE.
2. IT IS IMPLEMENTED IN PRODCUTIONS.
3. IN HA WE WILL NOT HAVE SECONDARY NAMENODE.
4. ONLY ONE NAMENODE WILL BE ACTIVE AND IF ACTIVE NAMENODE DIES AUTOMATICALLY STANDBY WILL BECOME NEW ACTIVE.
5. HA IS IMPPLEMENTED USING ADDITIONAL DAEMONS: JOURNAL NODES, ZOOKEEPER, ZKFC..
6. DATANODES WILL BE SENDING HEARTBEAT TO BOTH ACTIVE AND STANDBY NAMENODES.

HDFS COMMANDS
1. HDFS CAN BE ACCESSED USING "hdfs dfs" COMMANDS.
2. MOST OF THE COMMANDS RESEMBLES LINUX.

FILE PERMISSIONS
1. OWNER
2. GROUP
3. OTHERS

r --> read
w --> write
x --> execute





HDFS COMMANDS STARTS WITH hdfs dfs.

1. hdfs dfs (enter) --> List of all the options under hdfs dfs.

2. hdfs dfs -help <option>
Ex: hdfs dfs -help ls
    hdfs dfs -help put

3. copyFromLocal or put options helps in writing the file from local filesystem to hdfs. 

4. copyToLocal or get options helps in getting the file from hdfs to local system.

5. ls command helps in listing the files and directories under the location.

6. chmod command helps in changing the file permissions.

Owner Group Others
rwx   rwx   rwx

r (read): 4
w (write): 2
x (execute):1

rwx: 4+2+1: 7

Ex:
hdfs dfs -chmod 644 /user/training/ratings.csv

6: Owner --> rw
4: Group --> r
4: Others --> r

-rw-r--r--

7. rm option is to remove the file.
   rm -r option for removing the directory and its files.

8. cp option is to copy the file from one path to another within the hdfs.

9. cat option is to read the contents of the file.

YARN
HADOOP V1
1. ONLY MAPREDUCE

MAPREDUCE IS A PROCESSING TECNIQUE MOSTLY WRITTEN IN JAVA AND COMPILED AS JAR FILE.

MAP AND REDUCE ARE 2 DIFFERENT OPERATIONS

MAP SUFFLES AND ARRANGES DATA, REDUCER APPLIES AGGREGATION ON THE MAP DATA.

MAPREDUCE WORKS WITH THE HELP OF <KEY> <VALUE>

WE CAN HAVE MAP ONLY MAPREDUCE JOBS OR MAPREDUCE JOBS.

MAP ONLY MAPREDUCE JOBS
EX: SELECT * FROM EMPLOYEES;
    SELECT * FROM EMPLOYEES WHERE SALARY>2000;

MAPREDUCE JOB
EX: SELECT DEPTNO,SUM(SALARY) FROM EMPLOYEES GROUP BY DEPTNO ORDER BY SUM(SALARY) DESC;

MAPREDUCE IS A DISK PROCESSING, MEANS LOT OF INTERMEDIATE IS WRITTEN AND READ.
MAP AND REDUCE FUNCTIONS AS MPP (MASSIVE PARALLEL PROCESSING), WE CAN HAVE MULTIPLE MAPPERS AND REDUCERS FOR SINGLE JOB.
MAP OUTPUT FILE IS NAMED AS: part-m-00000
MAPREDUCE OUTPUT FILE IS NAMED AS: part-r-00000
MAPREDUCE JOBS DO NOT HAVE INTERACTIVE MODE HENCE SUBMITTED USING hadoop jar COMMAND.

2. JOB TRACKER/TASK TRACKER DAEMONS FOR MAPREDUCE.

3. MAPREDUCE IS SLOW IF THE DATA IS HUGE.

4. MAPREDUCE IS GOOD FOR BATCH JOBS BUT NOT COMPLEX AND STREAMS TYPE OF DATA.

HADOOP V2
1. YARN [YET ANOTHER RESOURCE NEGOTIATOR]
2. NO MONOPOLY OF MAPREDUCE, NOW MAPREDUCE WORKS UNDER YARN.
3. YARN IS HETEROGENIOUS IT SUPPORTS MULTIPLE DIFFERENT APPLICATIONS LIKE SPARK, SOLR, STORM, FLINK ETC TO RUN ON THE 
   CLUSTER.
4. YARN HELPS IN IMPLEMENTING MPP WITH FAULT TOLERANCE.
5. YARN ALSO HELPS IN CREATING SCHEDULING POLICIES FOR THE USERS.
6. RESOURCE MANAGER (MASTER) / NODE MANAGER (SLAVE)

HOW YARN WORKS?

1 JOB --> 1 APPLICATION

1 APPLICATION --> 1 APPLICATION MASTER

APPLICATION MASTER --> ONE OR MORE TASKS

A TASK IS A UNIT OF WORK TO BE DONE, AND MULTIPLE TASKS CAN RUN IN PARALLEL FOR A SINGLE APPLICATION [MPP]

NOTE: IN SPARK TASKS ARE HANDLED BY DRIVER PROGRAM.

APPLICATION MASTER AND TASKS RUNS WITHIN THE JVM, KNOWN AS CONTAINER

ANY JOB WHICH IS SUBMITTED WILL FIRST CONTACT RESOURCE MANAGER, AND RESOURCE MANAGER WILL CONTACT NODE MANAGERS TO CREATE CONTAINERS, GENERATE APPLICATION MASTER AND MONITOR THE CONTAINERS.

WE CAN SEE INFORMATION ABOUT THE JOBS AND TASKS FROM RESOURCE MANAGER WEBUI.

http://<hostname>:8088

NOTE: ONCE THE JOB/APPLICATION IS COMPLETED NODE MANAGERS CLEANS UP THE CONTAINERS.

DATA INGESTION

DATA --> STRUCTURED [RDBMS TABLES, SCHEMA FILES], SEMI STRUCTURED [LOG FILES,XML FILES, CSV FILES, SOCIAL MEDIA LIKE TWEETS],
         UNSTRUCTURED [STREAMS, ANALOG DATA, IMAGES & VIDEOS]


STRUCTURED DATA
1. SQOOP
IT IS A TOOL USED TO INGEST DATA FROM STRUCTURED SOURCES INTO HDFS.
SQOOP IS KNOWN AS IMPORT/EXPORT TOOL.
SQOOP IS A CLIENT WHICH COMMUNICATES WITH THE SOURCE USING DRIVER AND COMUNICATES WITH HDFS USING NAMENODE ADDRESS.
SQOOP INTERNALLY RUNS MAP ONLY MAPREDUCE JOB.
WE CAN USE DIFFERENT OPTIONS WITH SQOOP, AND CONDITIONS WHILE IMPORTING.
SQOOP IMPORTS THE TABLE AS DIRECTORY, AND ROWS DATA AS DELIMITED FILES UNDER THE DIRECTORY

DELIMITED DATA
1,Toy Story,1995
2,Jumanji,1995

1	Toy Story	1995
2	Jumanji	1995

1#Toy Story#1995
2#Jumanji#1995

SINCE SQOOP USES MAPREDUCE LOGIC THE DATA MUST HAVE KEY FOR IMPORT, HENCE THE TABLE PRIMARY KEY WILL BE USED AS KEY FOR THE DATA BY THE SQOOP.

NOTE: IF THERE IS NO PRIMARY KEY IN THE TABLE THEN WE HAVE TO USE --split-by or -m 1 OPTIONS.



FLUME
FLUME IS A TOOL TO GET SEMI STRUCTURED DATA LIKE LOGS, CSVS, SOCIAL MEDIA LIKE TWEETS INTO THE HDFS.
FLUME ACTUALLY ALSO HELPS IN LOGS AGGREGATION AND CAN INTEGRATE WITH OTHER TOOLS LIKE SPARK, KAFKA, NOSQL.

FILE CONFIGURATION
FLUME WORKS WITH THE HELP OF AGENT.
WE HAVE TO CREATE AGENT CONFIGURATION FILE.

IN THE CONFIGURATION FILE WE DEFINE AGENT NAME, SOURCE, CHANNEL, SINK 
ONCE CONFIGURATION FILE IS DEFINED, WE HAVE TO START THE FLUME AGENT

HIVE
DEVELOPED BY FACEBOOK.
HIVE IS A DWH OF HADOOP.
HIVE HELPS IN WRITING QUERIES ON THE STRUCTURED DATA LIKE TABLES AND WE CAN USE JOINS, SORTING, GROUPING ETC ON THE DATA.
HIVE USES A LANGUAGE CALLED HIVEQL, VERY SIMILAR TO SQL.
HIVE USES A METASTORE [DATABASE] TO STORE TABLE DEFINITIONS.
HIVE READS ACTUAL DATA FROM HDFS.
HIVE INTERNALLY USES MAPREDUCE, LATEST VERSIONS WE ALSO HAVE HIVE ON SPARK AND TEZ.
HIVE HELPS IN INTEGRATING THE RESULTS WITH VISUALIZATION TOOLS LIKE TABLEAU, QLIK VIEW, BI

CURRENTLY HIVE IS KNOWN AS HIVE2SERVER.
HIVE2SERVER IS A SERVICE WHICH IS A CENTRALIZED SERVICE FOR ALL THE DRIVERS AND ALL THE HIVE CLIENTS COMMUNICATES WITH HIVE2SERVER AND IN TURN HIVE2SERVER COMMUNICATES WITH METASTORE AND HDFS.




SPARK 
IT IS INDEPENDENT, GENERAL PURPOSE PROCESSING LANG., WRITTEN OVER FUNCTIONAL BASED PROG CALLED SCALA.
SPARK IS DISTRIBUTED MEMORY PROCESSING.
SPARK IS MPP WITH FAULT TOLERANCE.
SPARK SUPPORTS SEVERAL LANGUAGES APIs.

CORE SPARK --> SCALA
PYSPARK --> PYTHON
SPARKLYR --> R

SPARK ALSO SUPPORTS APIS WHICH CAN BE CALLED THROUGH SCALA OR PYTHON

1. SPARK SQL (DEFAULT FROM SPARK 2 VERSION)
2. SPARK JAVA --> FROM JAVA 1.8
3. SPARKML --> MACHINE LEARNING
4. SPARK STREAMS --> REAL TIME DATA STREAMS

SPARK ALSO HAVE SEVERAL MODES OF IMPLEMENTATION
LOCAL MODE --> SINGLE MACHINE (WINDOWS/MAC/LINUX) WITH NO CLUSTER AND NO HADOOP.
STANDALONE --> SPARK OWN CLUSTER, NO HADOOP. SPARK USES ITS OWN MASTER AND WORKER DAEMONS.
YARN MODE --> SPARK ON HADOOP [CLIENT MODE/CLUSTER MODE]
KUBERNATES --> SPARK ON DOCKERS OR CLOUD

SPARK CAN RUN BOTH AS INTERACTIVE MODE [REPL] OR BATCH MODE.

BATCH MODE MEANS WE CAN WRITE THE LOGIC IN A FILE AND SUBMIT THE FILE USING "spark-submit" COMMAND.

SCALA:  THE FILE MUST BE SAVED WITH .scala EXTENSION, THEN COMPILED USING IDE, MAVEN, OR SBT AND THEN GENERATE .jar FILE.
PYTHON: THE FILE MUST BE SAVED WITH .py EXTENSION, AND SUBMITTED TO THE COMMAND.

SPARK PERFORMS THE PROCESSING IN JVM CALLED "EXECUTORS".
SPARK RUNS AND INPUT/OUTPUT IO IS DONE BY A MAIN CLASS PROGRAM CALLED DRIVER.
EACH JOB OF SPARK IS ASSOCIATED WITH ITS OWN DRIVER PROGRAM.
NOTE: EXECUTORS OR DRIVER EXISTS ONLY UNTIL YOU ARE WITHIN THE SHELL OR A BATCH JOB IS RUNNING.

CLIENTS SUBMIT THE INPUT TO THE DRIVER THROUGH SPARK CONTEXT OBJECT.
sc (SPARK CONTEXT) --> Spark 1 version., now used for RDDs.
spark session (SPARK CONTEXT OBJECT) --> From Spark 2 version.


/user/ubh01/emp.txt

SCALA
val emprdd=sc.textFile("/user/ubh01/emp.txt")

val empDF=spark.read.textFile("/user/ubh01/emp.txt")



RDD --> RESILIENT DISTRIBUTED DATASETS
RESILIENCE MEANS RDD IS SELF HEALING, CAN AUTOMATICALLY TAKE REBIRTH IN CASH OF CRASH.
DATA OF THE FILE IS DISTRIBUTED AS MULTIPLE RDD PARTITIONS ACROSS EXECUTORS.
DATASET MEANS COLLECTION OF ARRAY OF RECORDS.

RDD IS IMMUTABLE.
RDD CAN BE TRANSFORMED INTO ANOTHER RDD.
RDD IS LAZY EXECUTION, MEANS UNTIL WE GIVE ACTION COMMANS RDD IS NOT CREATED BY SPARK.
BY DEFAULT ONCE OUTPUT IS GIVEN AFTER ACTION COMMAND, RDD IS REMOVED FROM THE MEMORY.

HOW RDD CAN BE CREATED?
1. FROM MEMORY (ON FLY)
2. FROM FILE
3. FROM ANOTHER RDD (TRANSFORMATION)
4. FROM DATAFRAMES






HIVE DML OPERATIONS

1. HIVE SUPPORTS DML OPERATIONS ONLY ON ORC FORMAT FILES.
DML (DATA MANUPLATION) --> INSERT, UPDATE, DELETE

2. ORC IS OPTIMIZED ROW COLUMNAR FORMAT.
IT IS APACHE OPEN SOURCE FORMAT.
SIMILAR TO COLUMNAR FORMAT.
DATA IS STORED AS STRIPES, WHICH CONTAINS THE INDEX, ROW DATA, STRIPE FOOTER AND MAPPED TO THE COLUMNS
DEFAULT SIZE OF THE STRIPE IS 250MB.
CONTAINS POSTSCRIPT WHICH HOLDS THE ENTIRE METADATA.

3. USING ORC FORMAT WE CAN PERFORM ACID OPERATIONS ON HIVE TABLE.
ACID --> ATOMICITY, CONSISTENCY, ISOLATION, AND DURABILITY

4. THERE IS NO COMMIT OR ROLLBACK.

5. EVEN THE FILE IS ORC WE CANNOT PERFORM DML OPERATIONS ON THE TABLE UNLESS WE SET SOME PRE-REQS.

A. CONCURRENCY PROPERTY MUST BE SET TO TRUE.
B. TRANSACTION MANAGER MUST BE ENABLED USING org.apache
C. DYNAMIC PARTITION MODE MUST BE SET TO "NONSTRICT"
D. WE HAVE TO SET TBLPROPERTIES AS "transactional" = "true" FOR THE TABLE ON WHICH WE NEED DML OPERATIONS.
E. TABLE MUST HAVE BUCKETS.


EXAMPLE:

hive> 
SET hive.support.concurrency=true;
SET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.dynamic.partition.mode=nonstrict;

CREATE TABLE EMPDMLS(EMPNO int, ENAME string, JOB string, MGR int, HIREDATE string, SALARY int, COMM int, DEPTNO int)
CLUSTERED BY(empno) INTO 2 BUCKETS
STORED AS ORC
TBLPROPERTIES("transactional"="true");

NOTE: TRANSACTIONAL TABLES CANNOT BE CREATED AS EXTERNAL TABLES.

INSERT INTO EMPDMLS SELECT * FROM EMP;







TEXTFILE
CSV
PARQUET
AVRO
ORC FILE

/user/root/ordersparquet/ordersinfo.parquet  (ordid,orname,dtoforder)

CREATE TABLE orders(ordid int,ordname string, dtoforder timestamp)
STORED AS PARQUET
row format delimited
fields terminated by ','
location '/users/root/ordersparquet/'

Hive partition
# cd Desktop/HandsOn_Hive
hdfs dfs -mkdir /user/root/depts

hdfs dfs -mkdir /user/root/depts/dept10
hdfs dfs -mkdir /user/root/depts/dept20
hdfs dfs -mkdir /user/root/depts/dept30

hdfs dfs -put dept10.txt /user/root/depts/dept10
hdfs dfs -put dept20.txt /user/root/depts/dept20
hdfs dfs -put dept30.txt /user/root/depts/dept30
 
beeline -u jdbc:hive2://elephant:10000/default -n root

create external table deptsnew(empno int,ename string,job string,mgr int,hiredate string,salary int,comm int,deptno int)
row format delimited
fields terminated by '/'
location '/user/root/depts';

> select * from depts;
(you must not see any rows displayed)

create external table deptspartempno int,ename string,job string,mgr int,hiredate string,salary int,comm int)
partitioned by (deptno string)
row format delimited
fields terminated by '/'
location '/user/root/depts';

ALTER TABLE deptspart ADD PARTITION (deptno='dept10') LOCATION '/user/root/depts/dept10';
ALTER TABLE deptspart ADD PARTITION (deptno='dept20') LOCATION '/user/root/depts/dept20';
ALTER TABLE deptspart ADD PARTITION (deptno='dept30') LOCATION '/user/root/depts/dept30';

BUCKETING IN HIVE
IT CAN DIVIDE THE DATA INTO GROUPS BASED ON THE CLUSTERING COLUMN AS BUCKETS.
BUCKET CAN BE USED WHERE PARTITIONING IS NOT POSSIBLE.

EMPNO

SELECT * FROM EMPLOYEES WHERE EMPNO IS BETWEEN 1 TO 100; --RANGE SEARCH

THE CONCEPT IF BUCKETING IS BASED ON THE HASHING TECHNIQUES.

BUCKETS ARE SUB DIRECTORIES CREATED BY HIVE, AND STORE THE RANGE OF THE DATA INTO RESPECTIVE SUB DIRECTORIES.


EXAMPLE

1. FIRST WE WILL CREATE A NORMAL TABLE AND THEN A BUCKETING TABLE AND TRANSFER THE NORMAL TABLE DATA INTO BUCKETING TABLE.

hive> create table emp(empno int,ename string,job string,mgr int,hiredate string,salary int,comm int,deptno int)
row format delimited
fields terminated by '/';

From Linux terminal
[root@saispark ~]# cd Desktop/
[root@saispark Desktop]# hdfs dfs -put emp.txt /user/hive/warehouse/emp/


hive> set hive.enforce.bucketing=true;


2. CREATING THE BUCKET TABLE

hive> create table emp_buckets(empno int,ename string,job string,mgr int,hiredate string,salary int,comm int,deptno int)
clustered by(empno) into 2 buckets
row format delimited
fields terminated by '/';

hive> insert overwrite table emp_buckets select * from emp;

$ hdfs dfs -ls /user/hive/warehouse/emp_buckets;

$ hdfs dfs -cat /user/hive/warehouse/emp_buckets/000000_0






















































